import os
import re
from pathlib import Path
import cv2
import torch
import numpy as np
import pandas as pd
from tqdm import tqdm
from collections import deque

# wilor_mini ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
from wilor_mini.pipelines.wilor_hand_pose3d_estimation_pipeline import WiLorHandPose3dEstimationPipeline

def natural_sort_key(s):
    """'frame1.jpg', 'frame2.jpg', 'frame10.jpg' ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤."""
    return [int(text) if text.isdigit() else text.lower()
            for text in re.split(r'([0-9]+)', str(s))]

# âœ… --- ì†ëª© íšŒì „ ë°©í–¥ íŠ¹ì§•ë§Œ ì¶”ì¶œí•˜ë„ë¡ í•¨ìˆ˜ ìˆ˜ì • ---
def extract_wrist_turn_feature_from_keypoints(
    keypoints_list: list,
    track_hand: str = 'right',
    num_frames: int = 500,
    smoothing_window_size: int = 5
):
    """
    ë©”ëª¨ë¦¬ì— ìˆëŠ” í‚¤í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ì†ëª© íšŒì „ ë°©í–¥ íŠ¹ì§•ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Returns:
        np.ndarray: (num_frames,) í˜•íƒœì˜ 1D íŠ¹ì§• ë²¡í„°.
    """
    # ì–‘ì†ì´ ëª¨ë‘ ê²€ì¶œëœ í”„ë ˆì„ì˜ í‚¤í¬ì¸íŠ¸ë§Œ í•„í„°ë§
    valid_keypoints = []
    for kps in keypoints_list:
        right_hand_kps = kps[:21, :]
        left_hand_kps = kps[21:, :]
        is_right_detected = not np.all(right_hand_kps == 0)
        is_left_detected = not np.all(left_hand_kps == 0)

        if is_right_detected and is_left_detected:
            if track_hand == 'right':
                valid_keypoints.append(right_hand_kps)
            else:
                valid_keypoints.append(left_hand_kps)

    if not valid_keypoints:
        tqdm.write(f"Warning: ìœ íš¨í•œ ì–‘ì† í‚¤í¬ì¸íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. 0 ë²¡í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
        return np.zeros(num_frames)

    # ì¤‘ì•™ì—ì„œ num_frames ë§Œí¼ ìë¥´ê¸°
    total_valid_frames = len(valid_keypoints)
    if total_valid_frames <= num_frames:
        keypoints_to_process = valid_keypoints
    else:
        start_index = (total_valid_frames - num_frames) // 2
        end_index = start_index + num_frames
        keypoints_to_process = valid_keypoints[start_index:end_index]

    # ì†ëª© íšŒì „ ë°©í–¥ ê³„ì‚°
    wrist_turn_history = []
    accumulated_turn_direction = 0.0
    wrist_pos_deque = deque(maxlen=3)

    for keypoints_3d in keypoints_to_process:
        current_wrist_pos = keypoints_3d[0]
        wrist_pos_deque.append(current_wrist_pos[:2]) # XY í‰ë©´ì—ì„œì˜ ì›€ì§ì„
        
        turn_direction = 0.0
        if len(wrist_pos_deque) == 3:
            p_prev, p_mid, p_curr = wrist_pos_deque
            v1, v2 = p_mid - p_prev, p_curr - p_mid
            # 2D ì™¸ì ì„ í†µí•´ íšŒì „ ë°©í–¥ íŒë‹¨
            cross_product_z = v1[0] * v2[1] - v1[1] * v2[0]
            turn_direction = np.sign(cross_product_z)
            
        accumulated_turn_direction += turn_direction
        wrist_turn_history.append(accumulated_turn_direction)

    # 1D ë²¡í„°ë¡œ ë³€í™˜
    feature_vector = np.array(wrist_turn_history)

    # íŒ¨ë”©
    if len(feature_vector) < num_frames:
        padding_value = feature_vector[-1] if len(feature_vector) > 0 else 0
        padding = np.full(num_frames - len(feature_vector), padding_value)
        feature_vector = np.concatenate([feature_vector, padding])

    # í‰í™œí™”
    if smoothing_window_size > 1 and len(feature_vector) > 0:
        feature_vector = pd.Series(feature_vector).rolling(
            window=smoothing_window_size, min_periods=1, center=True).mean().to_numpy()

    return feature_vector

def process_dataset_and_save_features():
    # --- âš™ï¸ ì‚¬ìš©ìê°€ ì„¤ì •í•  ë¶€ë¶„ âš™ï¸ ---
    base_input_folder = "/media/kcy/3A72CA7272CA3285/data_MHAV"
    track_hand = 'right'
    num_frames = 500
    smoothing_window = 5
    # ------------------------------------

    print("ğŸš€ WILOR ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”...")
    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    pipe = WiLorHandPose3dEstimationPipeline(device=device, dtype=torch.float16, verbose=False)
    print("âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ.")

    try:
        image_folders = sorted(Path(base_input_folder).glob('**/RGB_undistorted/processed_270_480'))
        action_folders = sorted(list(set([p.parent.parent for p in image_folders])))
        
        action_folders = [folder for folder in action_folders if 'unscrew' in folder.name]
    except FileNotFoundError:
        print(f"Error: ìµœìƒìœ„ ì…ë ¥ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {base_input_folder}")
        return
    if not action_folders:
        print(f"Error: '{base_input_folder}' ì•ˆì—ì„œ ì²˜ë¦¬í•  ì´ë¯¸ì§€ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"\nì´ {len(action_folders)}ê°œì˜ ì•¡ì…˜ í´ë”ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    for action_folder in tqdm(action_folders, desc="ì „ì²´ ì•¡ì…˜ ì§„í–‰ë¥ "):
        action_name = f"{action_folder.parent.name}_{action_folder.name}"
        tqdm.write(f"\nProcessing action: {action_name}")

        image_folder = action_folder / "RGB_undistorted/processed_270_480"
        image_paths = sorted(image_folder.glob('*.jpg'), key=natural_sort_key)
        
        if not image_paths:
            tqdm.write(f"Warning: '{action_name}'ì— ì´ë¯¸ì§€ê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
        
        all_frame_keypoints = []
        for img_path in tqdm(image_paths, desc=f"Pose Estimation ({action_name})", leave=False):
            image = cv2.imread(str(img_path))
            if image is None: continue

            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            outputs = pipe.predict(image_rgb)
            
            if not outputs:
                all_keypoints = np.zeros((42, 3), dtype=np.float32)
            else:
                missing_hand_placeholder = np.zeros((21, 3), dtype=np.float32)
                right_hand_kps, left_hand_kps = missing_hand_placeholder, missing_hand_placeholder
                for hand_data in outputs:
                    keypoints_full = hand_data['wilor_preds']['pred_keypoints_3d'][0]
                    keypoints = keypoints_full[:, :3]
                    if hand_data['is_right']: right_hand_kps = keypoints
                    else: left_hand_kps = keypoints
                all_keypoints = np.concatenate([right_hand_kps, left_hand_kps], axis=0)
            
            all_frame_keypoints.append(all_keypoints)
        
        # âœ… --- ì†ëª© íšŒì „ ë°©í–¥ íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜ í˜¸ì¶œ ---
        wrist_turn_feature = extract_wrist_turn_feature_from_keypoints(
            keypoints_list=all_frame_keypoints,
            track_hand=track_hand,
            num_frames=num_frames,
            smoothing_window_size=smoothing_window
        )
        
        # âœ… .npy íŒŒì¼ ì´ë¦„ ìˆ˜ì •
        output_filename = f"{action_name}_wrist_turn.npy"
        output_path = action_folder / output_filename
        np.save(output_path, wrist_turn_feature)
        tqdm.write(f"âœ… ì†ëª© íšŒì „ ë°©í–¥ íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ ë° ì €ì¥: {output_path}")

    print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == '__main__':
    process_dataset_and_save_features()